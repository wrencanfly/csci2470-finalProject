{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filepath = \"./instances_val2017.json\"\n",
    "with open(filepath, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def filter_data(data, min_id, max_id):\n",
    "    data_copy = copy.deepcopy(data)\n",
    "    # Filter categories\n",
    "    filtered_categories = [cat for cat in data_copy['categories'] if min_id <= cat['id'] <= max_id]\n",
    "    valid_ids = {cat['id'] for cat in filtered_categories}\n",
    "\n",
    "    # Filter annotations based on valid category_ids\n",
    "    filtered_annotations = [anno for anno in data_copy['annotations'] if anno['category_id'] in valid_ids]\n",
    "    valid_image_ids = {anno['image_id'] for anno in filtered_annotations}\n",
    "\n",
    "    # Filter images based on remaining valid image_ids\n",
    "    filtered_images = [img for img in data_copy['images'] if img['id'] in valid_image_ids]\n",
    "\n",
    "    # Update the data dictionary with filtered data\n",
    "    data_copy['categories'] = filtered_categories\n",
    "    data_copy['annotations'] = filtered_annotations\n",
    "    data_copy['images'] = filtered_images\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX is 79\n",
    "def gen_dataset(start_category_id, end_category_id, filepath = \"./instances_val2017.json\"):\n",
    "    def save_json(data, filename):\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "            \n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    filtered_data = filter_data(data, start_category_id, end_category_id)\n",
    "    save_json(filtered_data, f'filtered_data_{start_category_id}-{end_category_id}.json')\n",
    "    print('saved')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate IN dataset - include train and test, split it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "# now I will only use 50 total categories - 0~59\n",
    "# ID from 0~39\n",
    "# OOD from 40~59\n",
    "\n",
    "ID_RANGE = [0, 39]\n",
    "OOD_RANGE = [40, 59]\n",
    "\n",
    "# then split the ID into training and testing. OOD are all testing\n",
    "\n",
    "# This is the IN data, we will split the IN test data into train and test later\n",
    "import os\n",
    "gen_dataset(start_category_id=ID_RANGE[0], end_category_id=ID_RANGE[1])\n",
    "current_file_name = f'filtered_data_{ID_RANGE[0]}-{ID_RANGE[1]}.json'\n",
    "new_file_name = f'IN_data_{ID_RANGE[0]}-{ID_RANGE[1]}.json'\n",
    "os.rename(current_file_name, new_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate OOD test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "# Generate OOD test dataset - rename it as OOD_TEST_data_40-59.json\n",
    "import os\n",
    "gen_dataset(start_category_id=OOD_RANGE[0], end_category_id=OOD_RANGE[1])\n",
    "current_file_name = f'filtered_data_{OOD_RANGE[0]}-{OOD_RANGE[1]}.json'\n",
    "new_file_name = f'OOD_TEST_data_{OOD_RANGE[0]}-{OOD_RANGE[1]}.json'\n",
    "os.rename(current_file_name, new_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the IN data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "def split_data(data, ratio=0.8):\n",
    "    # Extract image IDs\n",
    "    image_ids = [img['id'] for img in data['images']]\n",
    "    \n",
    "    # Shuffle image IDs to randomize the split\n",
    "    random.shuffle(image_ids)\n",
    "    \n",
    "    # Calculate the split index\n",
    "    split_index = int(len(image_ids) * ratio)\n",
    "    \n",
    "    # Split image IDs into training and testing sets\n",
    "    train_image_ids = set(image_ids[:split_index])\n",
    "    test_image_ids = set(image_ids[split_index:])\n",
    "    \n",
    "    # Create train and test datasets based on image IDs\n",
    "    train_data = {\n",
    "        'images': [img for img in data['images'] if img['id'] in train_image_ids],\n",
    "        'annotations': [anno for anno in data['annotations'] if anno['image_id'] in train_image_ids],\n",
    "        'categories': data['categories']\n",
    "    }\n",
    "    test_data = {\n",
    "        'images': [img for img in data['images'] if img['id'] in test_image_ids],\n",
    "        'annotations': [anno for anno in data['annotations'] if anno['image_id'] in test_image_ids],\n",
    "        'categories': data['categories']\n",
    "    }\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "data = load_json('IN_data_0-39.json')\n",
    "\n",
    "# Split the data with a specific ratio\n",
    "train_data, test_data = split_data(data, ratio=0.8)\n",
    "\n",
    "# Save the split data\n",
    "save_json(train_data, 'IN_TRAIN_data_0-39.json')\n",
    "save_json(test_data, 'IN_TEST_data_0-39.json')\n",
    "\n",
    "print(\"Data has been split into training and testing sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have following datasets:\n",
    "- training data:\n",
    "    - IN_TRAIN_data_0-39.json\n",
    "- testing dataL\n",
    "    - IN_TEST_data_0-39.json\n",
    "    - OOD_TEST_data_40-59.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regionclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
